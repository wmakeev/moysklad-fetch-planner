import Moysklad from 'moysklad'
import { fetch } from 'undici'
import { FetchPlanner } from '../src/index.js'
import _H from 'highland'
import {
  orderlessParallel,
  promiseToStream,
  throughputProbe,
  ThroughputProbeEvent,
  ThroughputProbeHandler,
  isNil
} from '@wmakeev/highland-tools'
import assert from 'node:assert'
import * as csv from 'csv-stringify/sync'
import { writeFile, mkdir } from 'node:fs/promises'
import path from 'node:path'
import { Document, DocumentRef, DocumentCollection } from './types.js'

/** –ö–æ–ª-–≤–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ –¥–∞–Ω–Ω—ã—Ö */
const SAMPLE_DOCS_SIZE = 1000

/** –ö–æ–ª-–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –æ—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ */
const SAMPLE_BATCH_SIZE = 100

/** –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ */
const CASE_NUM = 1

/** –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏ –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ */
const MAX_BATCH_SIZE = 20

/** –ó–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ 1 —ç–ª–µ–º–µ–Ω—Ç –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ */
const SOURCE_DELAY_MS = 5

/** –õ–∏–º–∏—Ç –Ω–∞ –∫–æ–ª-–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ */
const PARALLEL_LIMIT = 4

/** –í–∫–ª—é—á–µ–Ω –ª–∏ —Ä–µ–∂–∏–º —Ç—Ä–∏–≥–≥–µ—Ä–∞ –ø–æ —Å–≤–æ–±–æ–¥–Ω—ã–º —Å–ª–æ—Ç–∞–º */
const IS_SLOT_WAIT_ON = true

const fetchPlanner = new FetchPlanner(fetch, {
  maxParallelLimit: PARALLEL_LIMIT
})

const ms = Moysklad({ fetch: fetchPlanner.getFetch() })

//#region –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞

const extractDocumentInfo = (doc: Document): DocumentRef => ({
  type: doc.meta.type,
  id: doc.id
})

console.log('Loading sample docs..')

// –ü–æ–ª—É—á–∏–º 1000 –∑–∞–∫–∞–∑–æ–≤ –∏ –≤–æ–∑—å–º–µ–º —É –Ω–∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
const ordersColl = (await ms.GET('entity/customerorder', {
  limit: SAMPLE_DOCS_SIZE
})) as DocumentCollection

const ordersInfo = ordersColl.rows.map(extractDocumentInfo)

//#endregion

async function sample1(items: DocumentRef[]) {
  const throughputEvents = [] as ThroughputProbeEvent[]

  const throughputHandler: ThroughputProbeHandler = ev => {
    const throughput = (ev.timeEnd - ev.timeStart) / ev.size

    console.log(
      `${ev.type === 'inbound' ? 'üîΩ' : 'üîº'} ${ev.label} ${ev.type} throughput ${throughput.toFixed(3)} ms/it`
    )

    throughputEvents.push(ev)
  }

  /**
   * –û–±—â–∏–π –ø–æ—Ç–æ–∫ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
   */
  const docs$ = _H(items)

  /**
   * –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–∫–∞–∑–æ–≤.
   * > –í –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ —ç—Ç–æ –Ω–µ –≤–∞–∂–Ω–æ, —Ç.–∫. –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞.
   */
  const orderIds$ = docs$.fork().filter(it => it.type === 'customerorder')

  /**
   * –ì—Ä—É–ø–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É:
   * - –≤ –≥—Ä—É–ø–ø–µ –Ω–µ –±–æ–ª—å—à–µ 50 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª-–≤–∞ –±—É–¥–µ—Ç —Å–ª–∏—à–∫–æ–º
   * –±–æ–ª—å—à–æ–π url —Ñ–∏–ª—å—Ç—Ä–∞)
   * - –æ–∂–∏–¥–∞–Ω–∏–µ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã –Ω–µ –¥–æ–ª—å—à–µ 1 —Å–µ–∫—É–Ω–¥—ã (–µ—Å–ª–∏ –∂–¥–∞—Ç—å —Å–ª–∏—à–∫–æ–º
   * –¥–æ–ª–≥–æ –º—ã —Ç–µ—Ä—è–µ–º –≤—Ä–µ–º—è –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞)
   */
  const ordersBatches$ = orderIds$
    .fork()
    .ratelimit(1, SOURCE_DELAY_MS)
    .consume<DocumentRef[]>(
      (() => {
        let consumeBuffer = [] as DocumentRef[]
        let isWaiting = false

        return (err, it, push, next) => {
          // error
          if (err) {
            push(err)
            next()
          }

          // end of stream
          else if (isNil(it)) {
            if (consumeBuffer.length > 0) {
              push(null, consumeBuffer)
            }
            consumeBuffer = []
            push(null, it)
          }

          // push data
          else {
            consumeBuffer.push(it)

            if (consumeBuffer.length >= MAX_BATCH_SIZE) {
              push(null, consumeBuffer)
              consumeBuffer = []
            }

            next()

            if (!isWaiting && IS_SLOT_WAIT_ON) {
              isWaiting = true

              fetchPlanner
                .waitForFreeRequestSlot()
                .then(() => {
                  isWaiting = false

                  const bufferLen = consumeBuffer.length

                  if (bufferLen > 0) {
                    push(null, consumeBuffer)
                    consumeBuffer = []
                  }
                })
                .catch((err: unknown) => {
                  console.log(err)
                })
            }
          }
        }
      })()
    )

  /**
   * –ü–æ—Ç–æ–∫ —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
   * - –∫–æ–ª-–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
   * - –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
   * - –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
   *
   * > –í—Ä–µ–º—è –º–æ–∂–µ—Ç –Ω–µ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞, —Ç.–∫. –∑–∞–ø—Ä–æ—Å
   * –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∫–∞–∫–æ–µ-—Ç–æ –≤—Ä–µ–º—è –≤ –æ—á–µ—Ä–µ–¥–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞.
   */
  const fetchedOrders$ = ordersBatches$
    .map(async batch => {
      assert.ok(batch[0])

      const type = batch[0].type

      console.log(`fetching batch of ${batch.length} items`)

      await fetchPlanner.waitForFreeRequestSlot()

      const docsColl = (await ms.GET(`entity/${type}`, {
        filter: {
          id: batch.map(it => it.id)
        }
      })) as DocumentCollection

      return docsColl.rows
    })
    .map(promiseToStream)

    // –ó–∞–ø—Ä–æ—Å—ã –¥–µ–ª–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º + 1, —á—Ç–æ–±—ã
    // –≤ –æ—á–µ—Ä–µ–¥–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –≤—Å–µ–≥–¥–∞ –±—ã–ª –æ–¥–∏–Ω –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å
    .through(orderlessParallel(PARALLEL_LIMIT + 1))
    .sequence()
    .through(
      throughputProbe('fetchedOrders$', 0, SAMPLE_BATCH_SIZE, throughputHandler)
    )

  const ids = await fetchedOrders$
    .map(it => it.id)
    .collect()
    .toPromise(Promise)

  assert.equal(ids.length, items.length)

  return throughputEvents
}

console.time(`Case ${CASE_NUM}`)
const stat = await sample1(ordersInfo)
console.timeEnd(`Case ${CASE_NUM}`)

const STAT_FOLDER_PATH = path.join(
  process.cwd(),
  `__temp/samples/waitForRequestSlot/PARALLEL_LIMIT=${PARALLEL_LIMIT}/BATCH_SIZE=${MAX_BATCH_SIZE}/`
)

await mkdir(STAT_FOLDER_PATH, { recursive: true })

await writeFile(
  path.join(STAT_FOLDER_PATH, 'fetched-docs.csv'),
  csv.stringify(stat)
)
